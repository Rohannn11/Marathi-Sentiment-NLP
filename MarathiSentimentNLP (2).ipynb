{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI2c6dd3QzBg"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn tensorflow nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/l3cube-pune/MarathiNLP/main/L3CubeMahaSent%20Dataset/tweets-train.csv\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Split into lines and print the first 5\n",
        "lines = response.text.splitlines()\n",
        "for i, line in enumerate(lines[:5]):\n",
        "    print(f\"Line {i}: {line}\")\n"
      ],
      "metadata": {
        "id": "UjmHOTZDRMi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/l3cube-pune/MarathiNLP/main/L3CubeMahaSent%20Dataset/tweets-train.csv\"\n",
        "df = pd.read_csv(url, encoding='ISO-8859-1')  # or 'latin1'\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "eZKp9zE6bX6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "\n",
        "def try_decode(text):\n",
        "    try:\n",
        "        return codecs.decode(text.encode('latin1'), 'utf-8')\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(try_decode)\n",
        "print(df['tweet'].head())\n"
      ],
      "metadata": {
        "id": "8OSzOamQbnrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape (rows, columns)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Check label distribution\n",
        "print(\"Label distribution:\\n\", df['label'].value_counts())"
      ],
      "metadata": {
        "id": "EU1UUBgkRQCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download 'punkt_tab' for sentence tokenization:\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download 'punkt' for word tokenization:\n",
        "nltk.download('punkt', force=True)\n",
        "\n",
        "# Verify downloads\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "    print(\"Punkt and Punkt_tab tokenizers are available.\")\n",
        "except LookupError:\n",
        "    print(\"Tokenizers not found. Re-downloading...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Load your dataset\n",
        "url = \"https://raw.githubusercontent.com/l3cube-pune/MarathiNLP/main/L3CubeMahaSent%20Dataset/tweets-train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Function to clean Marathi text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  # Keep Marathi characters\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "df['cleaned_tweet'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Tokenize\n",
        "df['tokens'] = df['cleaned_tweet'].apply(word_tokenize)\n",
        "\n",
        "# Display a sample\n",
        "print(df[['tweet', 'cleaned_tweet', 'tokens']].head())"
      ],
      "metadata": {
        "id": "v_yRzAOdRTzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!mkdir -p indic_nlp_resources/morph/morfessor\n",
        "!wget -O indic_nlp_resources/morph/morfessor/mr.model https://raw.githubusercontent.com/anoopkunchukuttan/indic_nlp_resources/master/morph/morfessor/mr.model\n"
      ],
      "metadata": {
        "id": "SQlV10GyTYTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install indic-nlp-library\n",
        "\n",
        "# Clone the indic_nlp_library repository (this contains the actual code)\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!cd indic_nlp_library && pip install -e .\n",
        "\n",
        "# Create directory for resources and download them\n",
        "!mkdir -p indic_nlp_resources\n",
        "!cd indic_nlp_resources && wget https://github.com/anoopkunchukuttan/indic_nlp_resources/archive/master.zip\n",
        "!cd indic_nlp_resources && unzip -o master.zip  # -o flag to overwrite without prompting\n",
        "!cd indic_nlp_resources && mv indic_nlp_resources-master/* .\n",
        "\n",
        "# Add the library to Python path so it can be imported\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the cloned repository directory to Python path\n",
        "repo_path = os.path.join(os.getcwd(), \"indic_nlp_library\")\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.insert(0, repo_path)\n",
        "\n",
        "# Now try importing\n",
        "try:\n",
        "    from indicnlp import common\n",
        "    from indicnlp.tokenize import indic_tokenize\n",
        "    from indicnlp.morph import unsupervised_morph\n",
        "    print(\"Successfully imported indicnlp modules!\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"Trying alternative import approach...\")\n",
        "\n",
        "    # Check if the package is installed and list its location\n",
        "    !pip show indic-nlp-library\n",
        "\n",
        "    # List directories to confirm structure\n",
        "    !ls -la\n",
        "    !ls -la indic_nlp_library\n",
        "\n",
        "    # Try installing directly from the repository again\n",
        "    !pip install -e indic_nlp_library\n",
        "\n",
        "    # Try importing again\n",
        "    try:\n",
        "        from indicnlp import common\n",
        "        from indicnlp.tokenize import indic_tokenize\n",
        "        from indicnlp.morph import unsupervised_morph\n",
        "        print(\"Successfully imported indicnlp modules after reinstallation!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Still having import issues: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Set up the environment\n",
        "RESOURCES_PATH = os.path.join(os.getcwd(), \"indic_nlp_resources\")\n",
        "os.environ[\"INDIC_RESOURCES_PATH\"] = RESOURCES_PATH\n",
        "common.set_resources_path(RESOURCES_PATH)\n",
        "\n",
        "# Ensure morfessor models directory exists\n",
        "morph_dir = os.path.join(RESOURCES_PATH, \"morph\", \"morfessor\")\n",
        "!mkdir -p {morph_dir}\n",
        "\n",
        "# Download Marathi morfessor model if it doesn't exist\n",
        "model_path = os.path.join(morph_dir, \"mr.model\")\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Downloading Marathi morfessor model...\")\n",
        "    !wget -O {model_path} https://raw.githubusercontent.com/anoopkunchukuttan/indic_nlp_resources/master/morph/morfessor/mr.model\n",
        "\n",
        "# Verify the model file exists\n",
        "!ls -la {model_path}\n",
        "\n",
        "# Install morfessor if not already installed\n",
        "!pip install morfessor\n",
        "\n",
        "# Initialize the unsupervised morph analyzer for Marathi\n",
        "print(\"Initializing morphological analyzer...\")\n",
        "try:\n",
        "    morph_analyzer = unsupervised_morph.UnsupervisedMorphAnalyzer('mr')\n",
        "    print(\"Morphological analyzer initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing morphological analyzer: {e}\")\n",
        "    # Show debug info\n",
        "    print(f\"Model path: {model_path}\")\n",
        "    print(f\"File exists: {os.path.exists(model_path)}\")\n",
        "    print(f\"File size: {os.path.getsize(model_path) if os.path.exists(model_path) else 'N/A'}\")\n",
        "    print(f\"INDIC_RESOURCES_PATH: {os.environ.get('INDIC_RESOURCES_PATH')}\")\n",
        "    raise\n",
        "\n",
        "# Function to get root words\n",
        "def get_root_words(text):\n",
        "    \"\"\"\n",
        "    Extract root words from Marathi text using morphological analysis.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input Marathi text\n",
        "\n",
        "    Returns:\n",
        "        list: List of root words\n",
        "    \"\"\"\n",
        "    words = indic_tokenize.trivial_tokenize(text)\n",
        "    root_words = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            analysis = morph_analyzer.morph_analyze(word)\n",
        "            if analysis and len(analysis) > 0:\n",
        "                # Get the root form (first element is usually the root)\n",
        "                root_words.append(analysis[0][0])\n",
        "            else:\n",
        "                root_words.append(word)\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing word '{word}': {str(e)}\")\n",
        "            root_words.append(word)\n",
        "    return root_words\n"
      ],
      "metadata": {
        "id": "nhFJi8EgVvmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"मी सकाळी लवकर उठलो आणि पुस्तक वाचले.\"\n",
        "print(\"\\nExample text:\", example_text)\n",
        "print(\"Tokenized:\", indic_tokenize.trivial_tokenize(example_text))\n",
        "print(\"Root words:\", get_root_words(example_text))\n",
        "\n",
        "print(\"\\nSetup complete! You can now use the get_root_words function on your dataframe.\")"
      ],
      "metadata": {
        "id": "6sdZ0vlJY287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['root_tokens'] = df['cleaned_tweet'].apply(get_root_words)"
      ],
      "metadata": {
        "id": "7qPrSb01ZErl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install fastText (run only if not already installed)\n",
        "!pip install fasttext -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import fasttext\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Verify that 'df' exists and has 'cleaned_tweet' column\n",
        "if 'df' not in globals():\n",
        "    raise NameError(\"DataFrame 'df' is not defined in the current environment. Please ensure it is loaded.\")\n",
        "if 'cleaned_tweet' not in df.columns:\n",
        "    raise KeyError(\"Column 'cleaned_tweet' not found in DataFrame 'df'. Please ensure it exists.\")\n",
        "\n",
        "# Function to prepare data for fastText training\n",
        "def prepare_fasttext_data(df, output_file='marathi_corpus.txt'):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for text in df['cleaned_tweet']:\n",
        "            f.write(text + '\\n')\n",
        "    return output_file\n",
        "\n",
        "# Generate corpus file\n",
        "corpus_file = prepare_fasttext_data(df)\n",
        "\n",
        "# Train fastText model on Marathi corpus\n",
        "model_file = 'marathi_fasttext.bin'\n",
        "model = fasttext.train_unsupervised(\n",
        "    corpus_file,\n",
        "    model='skipgram',\n",
        "    dim=100,  # Vector dimension\n",
        "    epoch=20,  # Number of epochs\n",
        "    minCount=5,  # Minimum word frequency\n",
        "    thread=4  # Number of threads for faster training\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save_model(model_file)\n",
        "\n",
        "# Function to get word vector\n",
        "def get_word_vector(word, model):\n",
        "    return model.get_word_vector(word)\n",
        "\n",
        "# Test the model with an example word\n",
        "example_word = \"खूप\"  # \"Very\" in Marathi\n",
        "vector = get_word_vector(example_word, model)\n",
        "print(f\"Vector for '{example_word}': {vector[:10]}... (first 10 dimensions)\")\n",
        "print(f\"Vector length: {len(vector)}\")"
      ],
      "metadata": {
        "id": "s7BowvBzc8RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers\n",
        "!pip install torch==2.3.0 transformers==4.41.2"
      ],
      "metadata": {
        "id": "GiVQyKB4bcb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4  # Compatible with transformers and torch"
      ],
      "metadata": {
        "id": "35bmS7TWbngy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ✅ Load dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/l3cube-pune/MarathiNLP/main/L3CubeMahaSent%20Dataset/tweets-train.csv\")\n",
        "\n",
        "# ✅ Load MahaMarathi BERT model\n",
        "model_name = \"l3cube-pune/marathi-bert-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
        "\n",
        "\n",
        "# Batch-wise detailed sentiment analysis function\n",
        "def detailed_sentiment_batch(texts, batch_size=10):\n",
        "    results = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize and move to device\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = sentiment_model(**inputs)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predictions = torch.argmax(probs, dim=1)\n",
        "\n",
        "        for j in range(len(batch)):\n",
        "            sentiment_score = predictions[j].item()\n",
        "            confidence_scores = probs[j].tolist()\n",
        "            results.append({\n",
        "                'sentiment': sentiment_score - 1,  # Convert to -1 (neg), 0 (neutral), 1 (positive)\n",
        "                'confidence': confidence_scores,\n",
        "                'intensity': abs(sentiment_score - 1),\n",
        "                'is_mixed': max(confidence_scores) < 0.6\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ✅ Apply sentiment analysis\n",
        "df['detailed_sentiment'] = detailed_sentiment_batch(df['tweet'].tolist(), batch_size=10)\n"
      ],
      "metadata": {
        "id": "RAOwTSRneN6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first 5 predictions\n",
        "df[['tweet', 'detailed_sentiment']].head()\n"
      ],
      "metadata": {
        "id": "fvRy30P11O3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment_label'] = df['detailed_sentiment'].apply(lambda x: x['sentiment'])\n",
        "df['sentiment_label'].value_counts()\n"
      ],
      "metadata": {
        "id": "A8JpPntA1tDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "69PxNoH_s-bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweet = \"हा अनुभव खूप छान होता!\"\n",
        "result = detailed_sentiment_batch([test_tweet])  # Pass as list\n",
        "print(\"Tweet:\", test_tweet)\n",
        "print(\"Result:\", result[0])  # Since result is a list of dicts\n"
      ],
      "metadata": {
        "id": "yykIkeVN1x5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].value_counts())\n"
      ],
      "metadata": {
        "id": "fN_hibdZ3CXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path where the model was saved in Google Drive\n",
        "model_path = '/content/drive/MyDrive/marathi_sentiment_finetuned'\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sentiment_model.to(device)\n",
        "\n",
        "print(\"Fine-tuned model loaded successfully!\")"
      ],
      "metadata": {
        "id": "WEkr-OPAU5tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade peft"
      ],
      "metadata": {
        "id": "L78FMXUmlCCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define a compute_metrics function to calculate accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)  # Get the predicted class (0, 1, 2)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Assuming trainer is still in scope from your fine-tuning step\n",
        "# If not, reinitialize it with the fine-tuned model\n",
        "\n",
        "# Define training arguments (minimal setup for evaluation)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./marathi_sentiment_model',  # Replace with your desired output directory\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=sentiment_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics  # Add the metrics function\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the results\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy'] * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "RjJREo-WdZBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7ruQhu0ch4fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = '/content/drive/MyDrive/marathi_sentiment_finetuned'  # Adjusted path to Google Drive\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sentiment_model.to(device)\n",
        "\n",
        "# Load the test dataset\n",
        "test_url = \"https://raw.githubusercontent.com/l3cube-pune/MarathiNLP/main/L3Cube-MahaSent-MD/MahaSent_All/MahaSent_All_Test.csv\"\n",
        "test_df = pd.read_csv(test_url)\n",
        "\n",
        "# Function to clean Marathi text (consistent with your training)\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  # Keep Marathi characters\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Check the column names in the test_df\n",
        "print(test_df.columns)\n",
        "# Assuming the tweet column is named \"text\" based on the file structure of MahaSent_All_Test.csv\n",
        "# Apply cleaning to the correct column (e.g., 'text' instead of 'tweet')\n",
        "test_df['cleaned_tweet'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "# Prepare test data\n",
        "test_texts = test_df['cleaned_tweet'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "\n",
        "# Convert labels to [0, 1, 2] format (same as training)\n",
        "test_labels = [l + 1 for l in test_labels]\n",
        "\n",
        "# Custom dataset class (from your notebook)\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# Define compute_metrics for accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)  # Predicted class (0, 1, 2)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    # Convert back to [-1, 0, 1] for detailed report\n",
        "    predictions_adjusted = [p - 1 for p in predictions]\n",
        "    labels_adjusted = [l - 1 for l in labels]\n",
        "    report = classification_report(labels_adjusted, predictions_adjusted, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)'], output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"classification_report\": report\n",
        "    }\n",
        "\n",
        "# Training arguments (minimal setup for evaluation)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./marathi_sentiment_model',  # Reuse from training\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Initialize trainer for evaluation\n",
        "trainer = Trainer(\n",
        "    model=sentiment_model,\n",
        "    args=training_args,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print results\n",
        "print(\"Test Set Evaluation Results:\")\n",
        "print(f\"Test Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy'] * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "for label, metrics in eval_results['eval_classification_report'].items():\n",
        "    if label in ['Negative (-1)', 'Neutral (0)', 'Positive (1)']:\n",
        "        print(f\"{label}: Precision: {metrics['precision']:.2f}, Recall: {metrics['recall']:.2f}, F1-Score: {metrics['f1-score']:.2f}\")"
      ],
      "metadata": {
        "id": "MDplbkn3h763"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweet = \"हा अनुभव खूप छान होता!\"\n",
        "result = detailed_sentiment_batch([test_tweet])  # Pass as list\n",
        "print(\"Tweet:\", test_tweet)\n",
        "print(\"Result:\", result[0])  # Since result is a list of dicts\n"
      ],
      "metadata": {
        "id": "SahfZ_0Tjic7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset).predictions.argmax(-1)\n",
        "misclassified = [(text, pred - 1, true - 1) for text, pred, true in zip(test_texts, predictions, test_labels) if pred != true]\n",
        "print(\"Misclassified Examples:\")\n",
        "for text, pred, true in misclassified[:5]:\n",
        "    print(f\"Tweet: {text}, Predicted: {pred}, True: {true}\")"
      ],
      "metadata": {
        "id": "0HaaPOWiknR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = './marathi_sentiment_finetuned'  # Adjust if saved elsewhere (e.g., Google Drive)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sentiment_model.to(device)\n",
        "\n",
        "# Load the test dataset\n",
        "test_url = \"https://raw.githubusercontent.com/l3cube-pune/MarathiNLP/main/L3Cube-MahaSent-MD/MahaSent_All/MahaSent_All_Test.csv\"\n",
        "test_df = pd.read_csv(test_url)\n",
        "\n",
        "# Function to clean Marathi text (consistent with your training)\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)  # Keep Marathi characters\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Check the column names in the test_df\n",
        "print(test_df.columns)\n",
        "# Assuming the tweet column is named \"text\" based on the file structure of MahaSent_All_Test.csv\n",
        "# Apply cleaning to the correct column (e.g., 'text' instead of 'tweet')\n",
        "test_df['cleaned_tweet'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "# Prepare test data\n",
        "test_texts = test_df['cleaned_tweet'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "\n",
        "# Convert labels to [0, 1, 2] format (same as training)\n",
        "test_labels = [l + 1 for l in test_labels]\n",
        "\n",
        "# Custom dataset class (from your notebook)\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# Define compute_metrics for accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)  # Predicted class (0, 1, 2)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    # Convert back to [-1, 0, 1] for detailed report\n",
        "    predictions_adjusted = [p - 1 for p in predictions]\n",
        "    labels_adjusted = [l - 1 for l in labels]\n",
        "    report = classification_report(labels_adjusted, predictions_adjusted, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)'], output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"classification_report\": report\n",
        "    }\n",
        "\n",
        "# Training arguments (minimal setup for evaluation)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./marathi_sentiment_model',  # Reuse from training\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Initialize trainer for evaluation\n",
        "trainer = Trainer(\n",
        "    model=sentiment_model,\n",
        "    args=training_args,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print results\n",
        "print(\"Test Set Evaluation Results:\")\n",
        "print(f\"Test Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy'] * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "for label, metrics in eval_results['eval_classification_report'].items():\n",
        "    if label in ['Negative (-1)', 'Neutral (0)', 'Positive (1)']:\n",
        "        print(f\"{label}: Precision: {metrics['precision']:.2f}, Recall: {metrics['recall']:.2f}, F1-Score: {metrics['f1-score']:.2f}\")"
      ],
      "metadata": {
        "id": "sXpM1rZJldHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = [\n",
        "    \"हा अनुभव खूप छान होता!\",  # Positive\n",
        "    \"माझा दिवस आज खूपच खराब गेला.\",  # Negative\n",
        "    \"हे ठिकाण अगदी ठिकठाक आहे.\",  # Neutral\n",
        "    \"तुमची सेवा खूपच चांगली आहे!\",  # Positive\n",
        "    \"हा चित्रपट वेळ वाया घालवणारा आहे.\",  # Negative\n",
        "    \"मी अजून निर्णय घेतलेला नाही.\",  # Neutral\n",
        "    \"माझं मन खूप आनंदी आहे!\",  # Positive\n",
        "    \"त्यांचं वागणं अजिबातच आवडलं नाही.\",  # Negative\n",
        "    \"आजचं हवामान साधारण आहे.\",  # Neutral\n",
        "    \"अशा अनुभवासाठी मी पुन्हा यायला तयार आहे!\",  # Positive\n",
        "    \"हे सॉफ्टवेअर खूप स्लो आहे.\",  # Negative\n",
        "    \"ते एक सामान्य उत्तर होतं.\",  # Neutral\n",
        "    \"मी तुमच्या सेवेने खूप प्रभावित झालो.\",  # Positive\n",
        "    \"खरंच वाईट अनुभव होता.\",  # Negative\n",
        "    \"काही खास नाही, नेहमीसारखंच होतं.\"  # Neutral\n",
        "]\n",
        "\n",
        "results = detailed_sentiment_batch(test_tweets)\n",
        "\n",
        "for tweet, result in zip(test_tweets, results):\n",
        "    print(\"Tweet:\", tweet)\n",
        "    print(\"Result:\", result)\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "utt6Tuz0nYZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Compare predictions of different models\n",
        "def plot_confusion_matrices(y_true, y_pred_fasttext, y_pred_llm, y_pred_ensemble):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # FastText confusion matrix\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred_fasttext), annot=True, fmt='d', ax=ax1)\n",
        "    ax1.set_title('FastText Model')\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('True')\n",
        "\n",
        "    # LLM confusion matrix\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred_llm), annot=True, fmt='d', ax=ax2)\n",
        "    ax2.set_title('MahaMarathi LLM')\n",
        "    ax2.set_xlabel('Predicted')\n",
        "    ax2.set_ylabel('True')\n",
        "\n",
        "    # Ensemble confusion matrix\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred_ensemble), annot=True, fmt='d', ax=ax3)\n",
        "    ax3.set_title('Ensemble Model')\n",
        "    ax3.set_xlabel('Predicted')\n",
        "    ax3.set_ylabel('True')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Analyze performance by sentiment category\n",
        "def plot_performance_by_category():\n",
        "    categories = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "    metrics = {'Precision': [], 'Recall': [], 'F1-Score': []}\n",
        "\n",
        "    for i, category in enumerate(categories):\n",
        "        report = classification_report(\n",
        "            df['label'] == i-1,\n",
        "\n",
        "            output_dict=True\n",
        "        )\n",
        "        metrics['Precision'].append(report['True']['precision'])\n",
        "        metrics['Recall'].append(report['True']['recall'])\n",
        "        metrics['F1-Score'].append(report['True']['f1-score'])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x - width, metrics['Precision'], width, label='Precision')\n",
        "    plt.bar(x, metrics['Recall'], width, label='Recall')\n",
        "    plt.bar(x + width, metrics['F1-Score'], width, label='F1-Score')\n",
        "\n",
        "    plt.xlabel('Sentiment Category')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Performance by Sentiment Category')\n",
        "    plt.xticks(x, categories)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute the visualization\n",
        "plot_performance_by_category()"
      ],
      "metadata": {
        "id": "pjfN2Qzhnws-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Replace 'ensemble_predictions' with the output of your ensemble model prediction\n",
        "# This is a placeholder and needs to be filled with your actual ensemble model logic\n",
        "ensemble_predictions = df['sentiment_label'].tolist()\n",
        "df['final_prediction'] = ensemble_predictions"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "b-8GAZzZryJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd1ddb54"
      },
      "source": [
        "!pip uninstall -y opencv-python opencv-python-headless\n",
        "!pip install torch==2.3.0 transformers==4.41.2 numpy==1.26.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e81f870f"
      },
      "source": [
        "!pip uninstall -y transformers peft\n",
        "!pip install transformers==4.41.2\n",
        "!pip install peft==0.17.1\n",
        "\n",
        "from transformers import Trainer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define a compute_metrics function to calculate accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)  # Get the predicted class (0, 1, 2)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Assuming trainer is still in scope from your fine-tuning step\n",
        "# If not, reinitialize it with the fine-tuned model\n",
        "trainer = Trainer(\n",
        "    model=sentiment_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics  # Add the metrics function\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the results\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy'] * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12c248b6"
      },
      "source": [
        "!pip check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1726a61"
      },
      "source": [
        "!pip uninstall -y thinc\n",
        "!pip install --upgrade --force-reinstall transformers==4.41.2\n",
        "!pip install --upgrade --force-reinstall peft==0.17.1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}